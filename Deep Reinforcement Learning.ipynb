{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8355c2e1",
   "metadata": {},
   "source": [
    "# Deep Reinforcement Learning - Cartpole Game (Code Along)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a511d966",
   "metadata": {},
   "source": [
    "Code Along with - https://www.youtube.com/watch?v=OYhFoMySoVs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d837f4eb",
   "metadata": {},
   "source": [
    "## Background"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f31fb6c8",
   "metadata": {},
   "source": [
    "*The Cartpole Game*  \n",
    "A pole is attached by an un-actuated joint to a cart, which moves along a frictionless track. Use the arrow keys to apply a force on the cart. The pendulum starts upright, and the goal is to prevent it from falling over. A score of +1 is provided for every move that the pole remains upright. The game ends when the pole is more than 15 degrees from vertical, or the cart touches the edges.  \n",
    "[Source](https://fluxml.ai/experiments/cartPole/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94f068d6",
   "metadata": {},
   "source": [
    "#### Important Terms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d3964bc",
   "metadata": {},
   "source": [
    "1. **Environment** - The World in which our agent is acting. Returns state and reward to **agent**\n",
    "2. **Agent** - The Actor that makes decisions in the environment. This will be what handles our model. Takes in **state** and **reward**, then returns **action**.\n",
    "3. **State** - The important variables in our environment. In this case, it will be cart position, cart velocity, pole angle, and pole angular velocity.\n",
    "4. **Reward** - Feedback for the model to determine how successful an action was. In this case, it will be trying to maximize the length of the game.\n",
    "5. **Action** - What the Agent does in response to the given environment. This will be what our model is attempting to predict. In this case, it will be deciding whether to move the cart left or right."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ec56b27",
   "metadata": {},
   "source": [
    "#### Markov Decision Process"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "947eaf88",
   "metadata": {},
   "source": [
    "+ **S** - All Possible States\n",
    "+ **A** - List of Possible Actions\n",
    "+ **R** - Reward Distribution given (s,a)\n",
    "+ **P** - Transition Probability Distribution of S[t+1] given (s,a)\n",
    "+ **D** - Discount Factor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e613dd8",
   "metadata": {},
   "source": [
    "*Objective is to get the Maximum Sum of D^t*r^t where t is the number of actions from the current state*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "403d8572",
   "metadata": {},
   "source": [
    "#### Q Learning Value Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a81c43f6",
   "metadata": {},
   "source": [
    "+ **Value Function (State)** - Expected reward given current state. In this situation, if the pole is completly vertical, this will be high, as the likelihood of success is high.\n",
    "+ **Q-Value Function (State, Action)** - Expected reward given current state and action. \n",
    "+ **Q*-Value Function (State, Action, Deep Q-Learning Network)** - Expected reward given current state and action through the lense of the Deep Q-Learning Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ee8c25a",
   "metadata": {},
   "source": [
    "## Dependencies, Environment, and Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cac54fd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"KERAS_BACKEND\"] = \"plaidml.keras.backend\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0d859da1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using plaidml.keras.backend backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import gym\n",
    "from collections import deque\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9a220eee",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e6bb84df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_size = env.observation_space.shape[0]\n",
    "state_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8a86f998",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action_size = env.action_space.n\n",
    "action_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c7cc7367",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5d0a45e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_episodes = 1001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d59ee410",
   "metadata": {},
   "outputs": [],
   "source": [
    "done = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "53be2f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = 'model_output/cartpole'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dd0680be",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3585b3cc",
   "metadata": {},
   "source": [
    "## Define Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8b97fa3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    \n",
    "    def __init__(self, state_size, action_size):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        \n",
    "        #Used for sampling from past experiences. This is important for making sure there is enough variety in the actions\n",
    "        self.memory = deque(maxlen=2000)\n",
    "        \n",
    "        self.gamma = 0.95\n",
    "        \n",
    "        #Helps balance exploitation vs exploration\n",
    "        self.epsilon = 1.0\n",
    "        self.epsilon_decay = 0.995\n",
    "        self.epsilon_min = 0.01\n",
    "        \n",
    "        #Step size for our optimizer\n",
    "        self.learning_rate = 0.001\n",
    "        \n",
    "        self.model = self._build_model()\n",
    "        \n",
    "    def _build_model(self):\n",
    "        \n",
    "        # Set up Model\n",
    "        model = Sequential()\n",
    "        \n",
    "        # Hidden Layers\n",
    "        model.add(Dense(24, input_dim = self.state_size, activation='relu'))\n",
    "        model.add(Dense(24, activation = 'relu'))\n",
    "        \n",
    "        # Output Layer\n",
    "        model.add(Dense(self.action_size, activation='linear'))\n",
    "        \n",
    "        #Compile Model\n",
    "        model.compile(loss='mse', optimizer=Adam(lr=self.learning_rate))\n",
    "        \n",
    "        return model\n",
    "    \n",
    "    #Create Datapoint for learning\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "    \n",
    "    #Determine explore or exploit\n",
    "    def act(self, state):\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return random.randrange(self.action_size)\n",
    "        act_values = self.model.predict(state)\n",
    "        return np.argmax(act_values[0])\n",
    "    \n",
    "    #Uses batch of memories to train the model\n",
    "    def replay(self, batch_size):\n",
    "        minibatch = random.sample(self.memory, batch_size)\n",
    "        for state, action, reward, next_state, done in minibatch:\n",
    "            target = reward\n",
    "            if not done:\n",
    "                target = (reward + self.gamma * np.amax(self.model.predict(next_state)[0]))\n",
    "            target_f = self.model.predict(state)\n",
    "            target_f[0][action] = target\n",
    "            \n",
    "            self.model.fit(state, target_f, epochs=1, verbose=0)\n",
    "            \n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "    \n",
    "    #Load model\n",
    "    def load(self,name):\n",
    "        self.model.load_weights(name)\n",
    "    \n",
    "    #Save Model\n",
    "    def save(self, name):\n",
    "        self.model.save_weights(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c950f28d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:plaidml:Opening device \"metal_amd_radeon_rx_5700_xt.0\"\n"
     ]
    }
   ],
   "source": [
    "agent = DQNAgent(state_size, action_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "affeb051",
   "metadata": {},
   "source": [
    "## Interact With Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "aa4b4c2c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 0/300, score: 42, e: 1.0\n",
      "episode: 1/300, score: 28, e: 0.99\n",
      "episode: 2/300, score: 14, e: 0.99\n",
      "episode: 3/300, score: 43, e: 0.99\n",
      "episode: 4/300, score: 25, e: 0.98\n",
      "episode: 5/300, score: 31, e: 0.98\n",
      "episode: 6/300, score: 19, e: 0.97\n",
      "episode: 7/300, score: 22, e: 0.97\n",
      "episode: 8/300, score: 12, e: 0.96\n",
      "episode: 9/300, score: 19, e: 0.96\n",
      "episode: 10/300, score: 13, e: 0.95\n",
      "episode: 11/300, score: 38, e: 0.95\n",
      "episode: 12/300, score: 37, e: 0.94\n",
      "episode: 13/300, score: 23, e: 0.94\n",
      "episode: 14/300, score: 20, e: 0.93\n",
      "episode: 15/300, score: 24, e: 0.93\n",
      "episode: 16/300, score: 12, e: 0.92\n",
      "episode: 17/300, score: 37, e: 0.92\n",
      "episode: 18/300, score: 20, e: 0.91\n",
      "episode: 19/300, score: 42, e: 0.91\n",
      "episode: 20/300, score: 36, e: 0.9\n",
      "episode: 21/300, score: 26, e: 0.9\n",
      "episode: 22/300, score: 33, e: 0.9\n",
      "episode: 23/300, score: 19, e: 0.89\n",
      "episode: 24/300, score: 10, e: 0.89\n",
      "episode: 25/300, score: 27, e: 0.88\n",
      "episode: 26/300, score: 69, e: 0.88\n",
      "episode: 27/300, score: 18, e: 0.87\n",
      "episode: 28/300, score: 13, e: 0.87\n",
      "episode: 29/300, score: 19, e: 0.86\n",
      "episode: 30/300, score: 12, e: 0.86\n",
      "episode: 31/300, score: 41, e: 0.86\n",
      "episode: 32/300, score: 58, e: 0.85\n",
      "episode: 33/300, score: 14, e: 0.85\n",
      "episode: 34/300, score: 22, e: 0.84\n",
      "episode: 35/300, score: 20, e: 0.84\n",
      "episode: 36/300, score: 13, e: 0.83\n",
      "episode: 37/300, score: 32, e: 0.83\n",
      "episode: 38/300, score: 29, e: 0.83\n",
      "episode: 39/300, score: 11, e: 0.82\n",
      "episode: 40/300, score: 12, e: 0.82\n",
      "episode: 41/300, score: 23, e: 0.81\n",
      "episode: 42/300, score: 17, e: 0.81\n",
      "episode: 43/300, score: 13, e: 0.81\n",
      "episode: 44/300, score: 43, e: 0.8\n",
      "episode: 45/300, score: 13, e: 0.8\n",
      "episode: 46/300, score: 16, e: 0.79\n",
      "episode: 47/300, score: 36, e: 0.79\n",
      "episode: 48/300, score: 16, e: 0.79\n",
      "episode: 49/300, score: 8, e: 0.78\n",
      "episode: 50/300, score: 17, e: 0.78\n",
      "episode: 51/300, score: 17, e: 0.77\n",
      "episode: 52/300, score: 21, e: 0.77\n",
      "episode: 53/300, score: 12, e: 0.77\n",
      "episode: 54/300, score: 32, e: 0.76\n",
      "episode: 55/300, score: 29, e: 0.76\n",
      "episode: 56/300, score: 59, e: 0.76\n",
      "episode: 57/300, score: 17, e: 0.75\n",
      "episode: 58/300, score: 21, e: 0.75\n",
      "episode: 59/300, score: 81, e: 0.74\n",
      "episode: 60/300, score: 18, e: 0.74\n",
      "episode: 61/300, score: 30, e: 0.74\n",
      "episode: 62/300, score: 23, e: 0.73\n",
      "episode: 63/300, score: 13, e: 0.73\n",
      "episode: 64/300, score: 61, e: 0.73\n",
      "episode: 65/300, score: 21, e: 0.72\n",
      "episode: 66/300, score: 69, e: 0.72\n",
      "episode: 67/300, score: 13, e: 0.71\n",
      "episode: 68/300, score: 51, e: 0.71\n",
      "episode: 69/300, score: 97, e: 0.71\n",
      "episode: 70/300, score: 34, e: 0.7\n",
      "episode: 71/300, score: 81, e: 0.7\n",
      "episode: 72/300, score: 22, e: 0.7\n",
      "episode: 73/300, score: 13, e: 0.69\n",
      "episode: 74/300, score: 41, e: 0.69\n",
      "episode: 75/300, score: 54, e: 0.69\n",
      "episode: 76/300, score: 24, e: 0.68\n",
      "episode: 77/300, score: 30, e: 0.68\n",
      "episode: 78/300, score: 88, e: 0.68\n",
      "episode: 79/300, score: 27, e: 0.67\n",
      "episode: 80/300, score: 29, e: 0.67\n",
      "episode: 81/300, score: 32, e: 0.67\n",
      "episode: 82/300, score: 13, e: 0.66\n",
      "episode: 83/300, score: 44, e: 0.66\n",
      "episode: 84/300, score: 14, e: 0.66\n",
      "episode: 85/300, score: 14, e: 0.65\n",
      "episode: 86/300, score: 17, e: 0.65\n",
      "episode: 87/300, score: 20, e: 0.65\n",
      "episode: 88/300, score: 50, e: 0.64\n",
      "episode: 89/300, score: 70, e: 0.64\n",
      "episode: 90/300, score: 50, e: 0.64\n",
      "episode: 91/300, score: 22, e: 0.63\n",
      "episode: 92/300, score: 97, e: 0.63\n",
      "episode: 93/300, score: 18, e: 0.63\n",
      "episode: 94/300, score: 23, e: 0.62\n",
      "episode: 95/300, score: 14, e: 0.62\n",
      "episode: 96/300, score: 24, e: 0.62\n",
      "episode: 97/300, score: 10, e: 0.61\n",
      "episode: 98/300, score: 10, e: 0.61\n",
      "episode: 99/300, score: 17, e: 0.61\n",
      "episode: 100/300, score: 14, e: 0.61\n",
      "episode: 101/300, score: 22, e: 0.6\n",
      "episode: 102/300, score: 9, e: 0.6\n",
      "episode: 103/300, score: 21, e: 0.6\n",
      "episode: 104/300, score: 20, e: 0.59\n",
      "episode: 105/300, score: 9, e: 0.59\n",
      "episode: 106/300, score: 18, e: 0.59\n",
      "episode: 107/300, score: 15, e: 0.58\n",
      "episode: 108/300, score: 15, e: 0.58\n",
      "episode: 109/300, score: 14, e: 0.58\n",
      "episode: 110/300, score: 13, e: 0.58\n",
      "episode: 111/300, score: 19, e: 0.57\n",
      "episode: 112/300, score: 27, e: 0.57\n",
      "episode: 113/300, score: 21, e: 0.57\n",
      "episode: 114/300, score: 13, e: 0.56\n",
      "episode: 115/300, score: 22, e: 0.56\n",
      "episode: 116/300, score: 17, e: 0.56\n",
      "episode: 117/300, score: 15, e: 0.56\n",
      "episode: 118/300, score: 19, e: 0.55\n",
      "episode: 119/300, score: 10, e: 0.55\n",
      "episode: 120/300, score: 11, e: 0.55\n",
      "episode: 121/300, score: 51, e: 0.55\n",
      "episode: 122/300, score: 40, e: 0.54\n",
      "episode: 123/300, score: 50, e: 0.54\n",
      "episode: 124/300, score: 70, e: 0.54\n",
      "episode: 125/300, score: 13, e: 0.53\n",
      "episode: 126/300, score: 28, e: 0.53\n",
      "episode: 127/300, score: 22, e: 0.53\n",
      "episode: 128/300, score: 16, e: 0.53\n",
      "episode: 129/300, score: 24, e: 0.52\n",
      "episode: 130/300, score: 29, e: 0.52\n",
      "episode: 131/300, score: 34, e: 0.52\n",
      "episode: 132/300, score: 14, e: 0.52\n",
      "episode: 133/300, score: 21, e: 0.51\n",
      "episode: 134/300, score: 25, e: 0.51\n",
      "episode: 135/300, score: 58, e: 0.51\n",
      "episode: 136/300, score: 36, e: 0.51\n",
      "episode: 137/300, score: 42, e: 0.5\n",
      "episode: 138/300, score: 25, e: 0.5\n",
      "episode: 139/300, score: 26, e: 0.5\n",
      "episode: 140/300, score: 12, e: 0.5\n",
      "episode: 141/300, score: 38, e: 0.49\n",
      "episode: 142/300, score: 18, e: 0.49\n",
      "episode: 143/300, score: 13, e: 0.49\n",
      "episode: 144/300, score: 16, e: 0.49\n",
      "episode: 145/300, score: 25, e: 0.48\n",
      "episode: 146/300, score: 17, e: 0.48\n",
      "episode: 147/300, score: 30, e: 0.48\n",
      "episode: 148/300, score: 18, e: 0.48\n",
      "episode: 149/300, score: 20, e: 0.47\n",
      "episode: 150/300, score: 13, e: 0.47\n",
      "episode: 151/300, score: 12, e: 0.47\n",
      "episode: 152/300, score: 10, e: 0.47\n",
      "episode: 153/300, score: 12, e: 0.46\n",
      "episode: 154/300, score: 14, e: 0.46\n",
      "episode: 155/300, score: 23, e: 0.46\n",
      "episode: 156/300, score: 18, e: 0.46\n",
      "episode: 157/300, score: 18, e: 0.46\n",
      "episode: 158/300, score: 15, e: 0.45\n",
      "episode: 159/300, score: 19, e: 0.45\n",
      "episode: 160/300, score: 11, e: 0.45\n",
      "episode: 161/300, score: 13, e: 0.45\n",
      "episode: 162/300, score: 28, e: 0.44\n",
      "episode: 163/300, score: 15, e: 0.44\n",
      "episode: 164/300, score: 18, e: 0.44\n",
      "episode: 165/300, score: 15, e: 0.44\n",
      "episode: 166/300, score: 11, e: 0.44\n",
      "episode: 167/300, score: 26, e: 0.43\n",
      "episode: 168/300, score: 19, e: 0.43\n",
      "episode: 169/300, score: 16, e: 0.43\n",
      "episode: 170/300, score: 15, e: 0.43\n",
      "episode: 171/300, score: 20, e: 0.42\n",
      "episode: 172/300, score: 20, e: 0.42\n",
      "episode: 173/300, score: 19, e: 0.42\n",
      "episode: 174/300, score: 37, e: 0.42\n",
      "episode: 175/300, score: 31, e: 0.42\n",
      "episode: 176/300, score: 170, e: 0.41\n",
      "episode: 177/300, score: 41, e: 0.41\n",
      "episode: 178/300, score: 73, e: 0.41\n",
      "episode: 179/300, score: 60, e: 0.41\n",
      "episode: 180/300, score: 34, e: 0.41\n",
      "episode: 181/300, score: 92, e: 0.4\n",
      "episode: 182/300, score: 199, e: 0.4\n",
      "episode: 183/300, score: 64, e: 0.4\n",
      "episode: 184/300, score: 50, e: 0.4\n",
      "episode: 185/300, score: 66, e: 0.4\n",
      "episode: 186/300, score: 46, e: 0.39\n",
      "episode: 187/300, score: 49, e: 0.39\n",
      "episode: 188/300, score: 47, e: 0.39\n",
      "episode: 189/300, score: 46, e: 0.39\n",
      "episode: 190/300, score: 25, e: 0.39\n",
      "episode: 191/300, score: 24, e: 0.38\n",
      "episode: 192/300, score: 29, e: 0.38\n",
      "episode: 193/300, score: 39, e: 0.38\n",
      "episode: 194/300, score: 34, e: 0.38\n",
      "episode: 195/300, score: 27, e: 0.38\n",
      "episode: 196/300, score: 59, e: 0.37\n",
      "episode: 197/300, score: 69, e: 0.37\n",
      "episode: 198/300, score: 76, e: 0.37\n",
      "episode: 199/300, score: 135, e: 0.37\n",
      "episode: 200/300, score: 63, e: 0.37\n",
      "episode: 201/300, score: 25, e: 0.37\n",
      "episode: 202/300, score: 34, e: 0.36\n",
      "episode: 203/300, score: 32, e: 0.36\n",
      "episode: 204/300, score: 19, e: 0.36\n",
      "episode: 205/300, score: 30, e: 0.36\n",
      "episode: 206/300, score: 26, e: 0.36\n",
      "episode: 207/300, score: 55, e: 0.35\n",
      "episode: 208/300, score: 64, e: 0.35\n",
      "episode: 209/300, score: 76, e: 0.35\n",
      "episode: 210/300, score: 55, e: 0.35\n",
      "episode: 211/300, score: 80, e: 0.35\n",
      "episode: 212/300, score: 27, e: 0.35\n",
      "episode: 213/300, score: 53, e: 0.34\n",
      "episode: 214/300, score: 62, e: 0.34\n",
      "episode: 215/300, score: 59, e: 0.34\n",
      "episode: 216/300, score: 56, e: 0.34\n",
      "episode: 217/300, score: 54, e: 0.34\n",
      "episode: 218/300, score: 22, e: 0.34\n",
      "episode: 219/300, score: 53, e: 0.33\n",
      "episode: 220/300, score: 39, e: 0.33\n",
      "episode: 221/300, score: 35, e: 0.33\n",
      "episode: 222/300, score: 43, e: 0.33\n",
      "episode: 223/300, score: 127, e: 0.33\n",
      "episode: 224/300, score: 68, e: 0.33\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 225/300, score: 65, e: 0.32\n",
      "episode: 226/300, score: 72, e: 0.32\n",
      "episode: 227/300, score: 82, e: 0.32\n",
      "episode: 228/300, score: 88, e: 0.32\n",
      "episode: 229/300, score: 95, e: 0.32\n",
      "episode: 230/300, score: 181, e: 0.32\n",
      "episode: 231/300, score: 97, e: 0.31\n",
      "episode: 232/300, score: 106, e: 0.31\n",
      "episode: 233/300, score: 36, e: 0.31\n",
      "episode: 234/300, score: 69, e: 0.31\n",
      "episode: 235/300, score: 99, e: 0.31\n",
      "episode: 236/300, score: 46, e: 0.31\n",
      "episode: 237/300, score: 199, e: 0.3\n",
      "episode: 238/300, score: 55, e: 0.3\n",
      "episode: 239/300, score: 52, e: 0.3\n",
      "episode: 240/300, score: 82, e: 0.3\n",
      "episode: 241/300, score: 72, e: 0.3\n",
      "episode: 242/300, score: 124, e: 0.3\n",
      "episode: 243/300, score: 64, e: 0.3\n",
      "episode: 244/300, score: 64, e: 0.29\n",
      "episode: 245/300, score: 63, e: 0.29\n",
      "episode: 246/300, score: 97, e: 0.29\n",
      "episode: 247/300, score: 88, e: 0.29\n",
      "episode: 248/300, score: 88, e: 0.29\n",
      "episode: 249/300, score: 65, e: 0.29\n",
      "episode: 250/300, score: 70, e: 0.29\n",
      "episode: 251/300, score: 94, e: 0.28\n",
      "episode: 252/300, score: 90, e: 0.28\n",
      "episode: 253/300, score: 63, e: 0.28\n",
      "episode: 254/300, score: 78, e: 0.28\n",
      "episode: 255/300, score: 81, e: 0.28\n",
      "episode: 256/300, score: 125, e: 0.28\n",
      "episode: 257/300, score: 75, e: 0.28\n",
      "episode: 258/300, score: 90, e: 0.27\n",
      "episode: 259/300, score: 122, e: 0.27\n",
      "episode: 260/300, score: 141, e: 0.27\n",
      "episode: 261/300, score: 82, e: 0.27\n",
      "episode: 262/300, score: 84, e: 0.27\n",
      "episode: 263/300, score: 120, e: 0.27\n",
      "episode: 264/300, score: 76, e: 0.27\n",
      "episode: 265/300, score: 89, e: 0.26\n",
      "episode: 266/300, score: 153, e: 0.26\n",
      "episode: 267/300, score: 134, e: 0.26\n",
      "episode: 268/300, score: 123, e: 0.26\n",
      "episode: 269/300, score: 90, e: 0.26\n",
      "episode: 270/300, score: 97, e: 0.26\n",
      "episode: 271/300, score: 55, e: 0.26\n",
      "episode: 272/300, score: 37, e: 0.26\n",
      "episode: 273/300, score: 103, e: 0.25\n",
      "episode: 274/300, score: 199, e: 0.25\n",
      "episode: 275/300, score: 81, e: 0.25\n",
      "episode: 276/300, score: 199, e: 0.25\n",
      "episode: 277/300, score: 102, e: 0.25\n",
      "episode: 278/300, score: 199, e: 0.25\n",
      "episode: 279/300, score: 180, e: 0.25\n",
      "episode: 280/300, score: 167, e: 0.25\n",
      "episode: 281/300, score: 152, e: 0.24\n",
      "episode: 282/300, score: 199, e: 0.24\n",
      "episode: 283/300, score: 114, e: 0.24\n",
      "episode: 284/300, score: 103, e: 0.24\n",
      "episode: 285/300, score: 105, e: 0.24\n",
      "episode: 286/300, score: 26, e: 0.24\n",
      "episode: 287/300, score: 54, e: 0.24\n",
      "episode: 288/300, score: 112, e: 0.24\n",
      "episode: 289/300, score: 144, e: 0.23\n",
      "episode: 290/300, score: 146, e: 0.23\n",
      "episode: 291/300, score: 133, e: 0.23\n",
      "episode: 292/300, score: 199, e: 0.23\n",
      "episode: 293/300, score: 184, e: 0.23\n",
      "episode: 294/300, score: 199, e: 0.23\n",
      "episode: 295/300, score: 122, e: 0.23\n",
      "episode: 296/300, score: 132, e: 0.23\n",
      "episode: 297/300, score: 126, e: 0.23\n",
      "episode: 298/300, score: 122, e: 0.22\n",
      "episode: 299/300, score: 199, e: 0.22\n"
     ]
    }
   ],
   "source": [
    "done = False\n",
    "\n",
    "for e in range(n_episodes): \n",
    "    #Reset Game\n",
    "    state = env.reset()\n",
    "    state = np.reshape(state, [1, state_size])\n",
    "    \n",
    "    for time in range(5000):  \n",
    "        #Render Environment\n",
    "        #env.render()\n",
    "        \n",
    "        #Get Action\n",
    "        action = agent.act(state)\n",
    "        \n",
    "        #Process Action in Environment\n",
    "        next_state, reward, done, _ = env.step(action) \n",
    "        \n",
    "        #Punish Failures\n",
    "        reward = reward if not done else -10     \n",
    "        \n",
    "        #Store Datapoint\n",
    "        next_state = np.reshape(next_state, [1, state_size])\n",
    "        agent.remember(state, action, reward, next_state, done) \n",
    "        \n",
    "        state = next_state    \n",
    "        if done: \n",
    "            print(\"episode: {}/{}, score: {}, e: {:.2}\" \n",
    "                  .format(e, n_episodes, time, agent.epsilon))\n",
    "            break\n",
    "        \n",
    "    #When Memory filled to batch size, train model on batch\n",
    "    if len(agent.memory) > batch_size:\n",
    "        agent.replay(batch_size) \n",
    "        \n",
    "    if e % 50 == 0:\n",
    "        agent.save(output_dir + \"weights_\" + '{:04d}'.format(e) + \".hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "822d3ad3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepLearning",
   "language": "python",
   "name": "deeplearning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
